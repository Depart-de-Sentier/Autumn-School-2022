{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39078190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uncertainty and global sensitivity analysis of LCA\n",
    "\n",
    "Author: [Aleksandra Kim](https://github.com/aleksandra-kim/)\n",
    "\n",
    "Brightway 2.5 Autumn school\n",
    "\n",
    "Grossh√∂chstetten, Switzerland, October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9be0a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Global sensitivity analysis of LCA\n",
    "\n",
    "<img src=\"input_data/gsa.png\" width=1600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd5519",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LCA case study\n",
    "\n",
    "- Environmental impacts of lithium carbonate production from Chaerhan brines. \n",
    "- Foreground inventories from [Schenker et al.](https://doi.org/10.1016/j.resconrec.2022.106611)\n",
    "- Uncertainty quantification in inputs is provided.\n",
    "- Numerically uncertainty propagation with Monte Carlo.\n",
    "- We will conduct uncertainty analysis ...\n",
    "- ... as well as global sensitivity analysis.\n",
    "- But GSA only for the foreground system in the interest of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af1359",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flowchart for the foreground inventories\n",
    "\n",
    "<img src=\"input_data/lithium_lci.png\" width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78297c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# === Switch to kernel bw25! ===\n",
    "\n",
    "# Brightway libraries\n",
    "import bw2data as bd\n",
    "import bw2io as bi\n",
    "import bw2calc as bc\n",
    "import bw_processing as bwp\n",
    "\n",
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fs.zipfs import ZipFS\n",
    "import json                        # Library for working with json files\n",
    "from pathlib import Path           # Library for working with paths in different OS     \n",
    "import matplotlib.pyplot as plt    # Library for creating plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1b14b",
   "metadata": {},
   "source": [
    "# 1. Setup BW project and databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create directories where we will be saving results\n",
    "output_dir = Path(f\"output_data\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# BW project\n",
    "project = \"Lithium GSA\"\n",
    "bd.projects.set_current(project)\n",
    "li_name = \"Chaerhan_38\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf9461",
   "metadata": {},
   "source": [
    "## Option 1 - import databases from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb95b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # === Import biosphere ===\n",
    "# bi.bw2setup()\n",
    "\n",
    "# # === Import ecoinvent ===\n",
    "# ei_path = \"/Users/akim/Documents/LCA_files/ecoinvent_38_cutoff/datasets\"\n",
    "# ei_name = \"ecoinvent 3.8 cutoff\"\n",
    "# if ei_name in bd.databases:\n",
    "#     print(\"ecoinvent database already exists\")\n",
    "# else:\n",
    "#     ei = bi.SingleOutputEcospold2Importer(ei_path, ei_name)\n",
    "#     ei.apply_strategies()\n",
    "#     assert ei.all_linked\n",
    "#     ei.write_database()\n",
    "    \n",
    "# # === Import foreground databases ===\n",
    "# # == Water database ==\n",
    "# if \"Water_38\" in bd.databases:\n",
    "#     print(\"Water_38 database already exists\")\n",
    "# else:\n",
    "#     # 1. Specify filepath to your foreground inventories.\n",
    "#     water_path = \"/srv/data/Water_database_38.xlsx\"\n",
    "#     # 2. Create an instance of a class that contains basic methods for importing a database from an excel file.\n",
    "#     water = bi.ExcelImporter(water_path)  \n",
    "#     # 3. `apply_strategies` is one of such basic methods, it makes sure units, locations, etc are in correct format.\n",
    "#     water.apply_strategies()\n",
    "#     # 4. Next step is to link your foreground exchanges to existing databases by matching relevant exchanges fields.\n",
    "#     water.match_database(\"biosphere3\", fields=(\"name\", \"unit\", \"categories\"))\n",
    "#     water.match_database(\"ecoinvent 3.8 cutoff\", fields=(\"name\", \"location\", \"unit\"))\n",
    "#     water.metadata.pop(None)  # Remove metadata None entry. TODO\n",
    "#     # 5. If everything is linked, write database so that it is saved in your project.\n",
    "#     if water.all_linked:\n",
    "#         water.write_database()\n",
    "        \n",
    "# # == Lithium database ==\n",
    "# # Do the same steps for the chosen Lithium database\n",
    "# if li_name in bd.databases:\n",
    "#     print(f\"{li_name} database already exists\")\n",
    "# else:\n",
    "#     lithium_path = f\"/srv/data/Chaerhan_database_38.xlsx\"\n",
    "#     lithium = bi.ExcelImporter(lithium_path)  \n",
    "#     lithium.apply_strategies()\n",
    "#     lithium.match_database(\"biosphere3\", fields=(\"name\", \"unit\", \"categories\"))\n",
    "#     lithium.match_database(\"ecoinvent 3.8 cutoff\", fields=(\"name\", \"location\", \"unit\")) \n",
    "#     # We also need to link Lithium database to the Water database\n",
    "#     lithium.match_database(\"Water_38\", fields=(\"name\", \"location\", \"unit\")) \n",
    "#     lithium.metadata.pop(None)\n",
    "#     if lithium.all_linked:\n",
    "#         lithium.write_database()\n",
    "\n",
    "# # === Project backup ===\n",
    "# # To backup a project use this command. Note that it will save project in your home directory!\n",
    "# bi.backup_project_directory(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe914fa0",
   "metadata": {},
   "source": [
    "## Option 2 - restore existing project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d32b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi.restore_project_directory(f\"/srv/data/brightway2-project-{project}-backup.25-October-2022-10-23PM.tar.gz\")\n",
    "bd.projects.set_current(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d6246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bd.databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae306c75",
   "metadata": {},
   "source": [
    "# 2. Deterministic LCIA score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1783747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's choose product for LCA\n",
    "lithium = bd.Database(li_name)\n",
    "demand_act = [act for act in lithium if \"Rotary dryer\" in act['name']][0]\n",
    "functional_unit = {demand_act: 1}  # functional unit\n",
    "\n",
    "# 2. And impact assessment method\n",
    "method = ('IPCC 2013', 'climate change', 'GWP 100a')\n",
    "\n",
    "# 3. Create `lca` object that contains necessary methods for LCI and LCIA \n",
    "lca = bc.LCA(functional_unit, method)\n",
    "\n",
    "# 4. Solve life cycle inventory problem\n",
    "lca.lci()\n",
    "\n",
    "# 5. Compute LCIA score\n",
    "lca.lcia()\n",
    "\n",
    "# 6. Returns the score, i.e. the sum of the characterized inventory\n",
    "deterministic_score = lca.score\n",
    "\n",
    "deterministic_score, bd.Method(method).metadata['unit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5829b9",
   "metadata": {},
   "source": [
    "# 3. Uncertainty analysis\n",
    "\n",
    "Propagate uncertainties in exchanges to LCIA scores. We use Brightway 25 datapackages for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1838bd2",
   "metadata": {},
   "source": [
    "## Consider 3 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "me = bd.Method(method).datapackage()\n",
    "bs = bd.Database(\"biosphere3\").datapackage()\n",
    "ei = bd.Database(\"ecoinvent 3.8 cutoff\").datapackage()\n",
    "wa = bd.Database(\"Water_38\").datapackage()\n",
    "li = bd.Database(li_name).datapackage()\n",
    "\n",
    "# Filtered datapackages that do not contain uncertainties\n",
    "ei_nounct = ei.exclude({\"kind\": \"distributions\"})\n",
    "wa_nounct = wa.exclude({\"kind\": \"distributions\"})\n",
    "li_nounct = li.exclude({\"kind\": \"distributions\"})\n",
    "\n",
    "# Case 1: uncertainties are present in both background and foreground\n",
    "dps_fb = [me, bs, ei, wa, li]\n",
    "\n",
    "# Case 2: uncertainties are present ONLY in FOREground\n",
    "dps_fg = [me, bs, ei_nounct, wa, li]\n",
    "\n",
    "# Case 3: uncertainties are present ONLY in BACKground\n",
    "dps_bg = [me, bs, ei, wa_nounct, li_nounct]\n",
    "\n",
    "cases = {\n",
    "    \"foreground_background\": {\"datapackages\": dps_fb}, \n",
    "    \"foreground\": {\"datapackages\": dps_fg}, \n",
    "    \"background\": {\"datapackages\": dps_bg}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f00348",
   "metadata": {},
   "source": [
    "## Run Monte Carlo simulations for the three cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565adb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "iterations_mc = 1000\n",
    "seed_mc = 5555  # specifying random seeds is needed for reproducibility of results\n",
    "\n",
    "# Run MC simulations for all cases\n",
    "for case, dict_ in cases.items():\n",
    "    print(case)\n",
    "    dps = dict_[\"datapackages\"]\n",
    "    lca_mc = bc.LCA(\n",
    "        {demand_act.id: 1}, \n",
    "        data_objs=dps,\n",
    "        use_distributions=True, \n",
    "        seed_override=seed_mc,\n",
    "    )\n",
    "    lca_mc.lci()\n",
    "    lca_mc.lcia()\n",
    "\n",
    "    # Since MC takes time, make sure to save the results!\n",
    "    # Also, read lca_scores from file if you already computed them once\n",
    "    fp_mc = output_dir / f'lca_scores_{seed_mc}_{iterations_mc}_{case}.json'\n",
    "\n",
    "    if fp_mc.exists():\n",
    "        # Read LCIA scores\n",
    "        with open(fp_mc, 'r') as f:\n",
    "            lca_scores_mc = json.load(f)\n",
    "    else:\n",
    "        # Run Monte Carlo simulations\n",
    "        lca_scores_mc = []\n",
    "        for i in range(iterations_mc):\n",
    "            next(lca_mc)\n",
    "            lca_scores_mc.append(lca_mc.score)\n",
    "        # Save LCIA scores\n",
    "        with open(fp_mc, 'w') as f:\n",
    "            json.dump(lca_scores_mc, f)\n",
    "     \n",
    "    dict_['lca_scores_mc'] = lca_scores_mc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd085f",
   "metadata": {},
   "source": [
    "## Plot distribution of LCIA scores\n",
    "\n",
    "TL;DR:  Use plotly instead of matplotlib!\n",
    "\n",
    "Long version:  These are not most beautiful plots, I highly recommend using [plotly package](https://plotly.com/python/) for visualizations. I believe, it is more intuitive and logical than matplotlib, but more importantly it allows interactive plots, where one can zoom in and out, hide some parts of plots, see what are the exact values of datapoints, etc. In this exercise I couldn't use plotly, because we are running notebooks on the servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae536d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "lca_scores_mc_all = np.hstack([dict_['lca_scores_mc'] for dict_ in cases.values()])\n",
    "lca_scores_mc_all = lca_scores_mc_all[\n",
    "    np.logical_and(\n",
    "        lca_scores_mc_all > np.percentile(lca_scores_mc_all, 0),\n",
    "        lca_scores_mc_all < np.percentile(lca_scores_mc_all, 99),\n",
    "    )\n",
    "]\n",
    "bins = np.linspace(min(lca_scores_mc_all), max(lca_scores_mc_all), num_bins, endpoint=True)\n",
    "midbins = (bins[:-1] + bins[1:]) / 2\n",
    "width = (bins[1]-bins[0])*0.8\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "plt.plot(\n",
    "    [deterministic_score],\n",
    "    [0],\n",
    "    \"rx\",\n",
    "    label=\"Deterministic LCIA score\",\n",
    ")\n",
    "\n",
    "# Uncertainty distributions for all cases\n",
    "for case, dict_ in cases.items():\n",
    "    lca_scores_mc = dict_['lca_scores_mc']\n",
    "    freq, _ = np.histogram(lca_scores_mc, bins=bins, density=False)\n",
    "    plt.bar(midbins, freq, width=width, label=case, alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"LCIA scores, kg CO2-eq\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d57cd3",
   "metadata": {},
   "source": [
    "# 4. Global sensitivity analysis for the foreground system\n",
    "\n",
    "There are a lot more uncertainties in the foreground, so it's alright to do GSA for the foreground only.\n",
    "\n",
    "GSA means that we need to compute quantitative measures of input importances for all model inputs. These measures are known as sensitivity indices.\n",
    "\n",
    "<img src=\"input_data/gsa_indices.png\" width=400 style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613ddc0",
   "metadata": {},
   "source": [
    "## Create `X` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrix_utils.resource_group import FakeRNG\n",
    "\n",
    "def create_X(lca_obj, nsamples, matrix_type, name, seed):\n",
    "    dp = bwp.create_datapackage(\n",
    "        name=name,\n",
    "        seed=seed,\n",
    "        sequential=True,\n",
    "    )\n",
    "    \n",
    "    num_resources = 3\n",
    "    if matrix_type == \"technosphere\":\n",
    "        num_resources = 4\n",
    "\n",
    "    obj = getattr(lca_obj, f\"{matrix_type}_mm\")\n",
    "\n",
    "    indices_array = np.hstack([\n",
    "        group.package.data[0] for group in obj.groups\n",
    "        if (not isinstance(group.rng, FakeRNG)) and (not group.empty) and (len(group.package.data) == num_resources)\n",
    "    ])\n",
    "\n",
    "    mask = np.ones(len(indices_array), dtype=bool)\n",
    "\n",
    "    data = []\n",
    "    np.random.seed(seed)\n",
    "    for _ in range(nsamples):\n",
    "        next(obj)\n",
    "        idata = []\n",
    "        for group in obj.groups:\n",
    "            if (not isinstance(group.rng, FakeRNG)) and (not group.empty):\n",
    "                idata.append(group.rng.random_data)\n",
    "        data.append(np.hstack(idata)[mask])\n",
    "    data_array = np.vstack(data).T\n",
    "    \n",
    "    if matrix_type == \"technosphere\":\n",
    "        flip_array = np.hstack([\n",
    "            group.flip for group in obj.groups\n",
    "            if (not isinstance(group.rng, FakeRNG)) and (not group.empty) and (len(group.package.data) == num_resources)\n",
    "        ])\n",
    "        dp.add_persistent_array(\n",
    "            matrix=f\"{matrix_type}_matrix\",\n",
    "            data_array=data_array,\n",
    "            name=name,\n",
    "            indices_array=indices_array[mask],\n",
    "            flip_array=flip_array[mask],\n",
    "        )\n",
    "    else:\n",
    "        dp.add_persistent_array(\n",
    "            matrix=f\"{matrix_type}_matrix\",\n",
    "            data_array=data_array,\n",
    "            name=name,\n",
    "            indices_array=indices_array[mask],\n",
    "        )\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01187d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X matrix\n",
    "dps_foreground = cases[\"foreground\"]['datapackages']  # only foreground should vary\n",
    "iterations_gsa = 1000\n",
    "seed_gsa = 7777\n",
    "\n",
    "lca_gsa = bc.LCA(\n",
    "    {demand_act.id: 1}, \n",
    "    data_objs=dps_foreground,\n",
    "    use_distributions=True,\n",
    "    seed_override=seed_gsa,\n",
    ")\n",
    "lca_gsa.lci()\n",
    "lca_gsa.lcia()\n",
    "\n",
    "dp_name_gsa = \"water_chaerhan\"\n",
    "dp_gsa_tech = create_X(lca_gsa, iterations_gsa, \"technosphere\", dp_name_gsa, seed_gsa)\n",
    "dp_gsa_bio = create_X(lca_gsa, iterations_gsa, \"biosphere\", dp_name_gsa, seed_gsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60bc238",
   "metadata": {},
   "source": [
    "## Run MC simulations, in other words - compute `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc2ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gsa_path = output_dir / f'lca_scores_{seed_gsa}_{iterations_gsa}_gsa.json'\n",
    "\n",
    "if gsa_path.exists():\n",
    "    # Read LCIA scores\n",
    "    with open(gsa_path, 'r') as f:\n",
    "        lca_scores_gsa = json.load(f)\n",
    "else:\n",
    "    # Run Monte Carlo simulations\n",
    "    dps_gsa = [me, bs, ei_nounct, wa_nounct, li_nounct, dp_gsa_tech, dp_gsa_bio]\n",
    "    lca_gsa = bc.LCA(\n",
    "        {demand_act.id: 1}, \n",
    "        data_objs=dps_gsa,\n",
    "        use_distributions=False, \n",
    "        use_arrays=True,\n",
    "    )\n",
    "\n",
    "    lca_gsa.lci()\n",
    "    lca_gsa.lcia()\n",
    "    lca_scores_gsa = [lca_gsa.score]\n",
    "    for i in range(iterations_gsa-1):\n",
    "        next(lca_gsa)\n",
    "        lca_scores_gsa.append(lca_gsa.score)\n",
    "    # Save LCIA scores\n",
    "    with open(gsa_path, 'w') as f:\n",
    "        json.dump(lca_scores_gsa, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c921258",
   "metadata": {},
   "source": [
    "## Compute GSA indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "# Let's collect X and y values\n",
    "X = np.vstack([dp_gsa_tech.data[1], dp_gsa_bio.data[1]]).T\n",
    "y = np.array(lca_scores_gsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dea3b",
   "metadata": {},
   "source": [
    "### First, check linearity of the model to decide which GSA method to choose\n",
    "\n",
    "Use standardized regression coefficients (SRC) to understand which GSA method is suitable.\n",
    "\n",
    "If sum of SRC is close to 1, then the model is approximately linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_test_ratio = 0.8\n",
    "split = int(train_test_ratio*iterations_gsa)\n",
    "Xtrain = X[:split, :]\n",
    "ytrain = y[:split]\n",
    "Xtest = X[split:, :]\n",
    "ytest = y[split:]\n",
    "\n",
    "reg = LinearRegression().fit(Xtrain, ytrain)\n",
    "# High reg.score in linear regression means that the model performs well on the train and test data.\n",
    "print(reg.score(Xtrain, ytrain), reg.score(Xtest, ytest))\n",
    "\n",
    "stdX = np.std(X, axis=0)\n",
    "stdy = np.std(y)\n",
    "\n",
    "src_coef = reg.coef_ * stdX / stdy\n",
    "src_sum = sum((src_coef)**2)\n",
    "src_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1159b",
   "metadata": {},
   "source": [
    "### Since this LCA model is somewhat linear, compute Spearman correlations\n",
    "\n",
    "These are GSA indices appropriate for the linear case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = []\n",
    "for x in X.T:\n",
    "    if len(set(x)) != 1:\n",
    "        spearman.append(spearmanr(x, y)[0])\n",
    "    else:\n",
    "        spearman.append(0)\n",
    "spearman = np.array(spearman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9415c",
   "metadata": {},
   "source": [
    "## Save GSA results in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.hstack([dp_gsa_tech.data[0], dp_gsa_bio.data[0]])\n",
    "data = np.vstack([dp_gsa_tech.data[1], dp_gsa_bio.data[1]]).T\n",
    "\n",
    "# Sort inputs (exchanges) by importance. Higher spearman correlation values mean higher importance of an input.\n",
    "spearman_argsort = np.argsort(spearman)[-1::-1]\n",
    "inds_ranked = inds[spearman_argsort]\n",
    "\n",
    "row_act_names, row_act_locations, row_act_categories = [], [], [] \n",
    "col_act_names, col_act_locations, static_data = [], [], []\n",
    "for i in inds:\n",
    "    row_act = bd.get_activity(i['row'])\n",
    "    col_act = bd.get_activity(i['col'])\n",
    "    row_act_names.append(row_act['name'])\n",
    "    row_act_locations.append(row_act.get(\"location\", \"None\"))\n",
    "    row_act_categories.append(row_act.get(\"category\", \"None\"))\n",
    "    col_act_names.append(col_act['name'])\n",
    "    col_act_locations.append(col_act.get(\"location\", \"None\"))\n",
    "    for exc in col_act.exchanges():\n",
    "        if row_act.id == exc.input.id:\n",
    "            static_data.append(exc.amount)\n",
    "            break\n",
    "static_data = np.array(static_data)\n",
    "\n",
    "dict_ = {\n",
    "    \"input_names\": row_act_names,\n",
    "    \"input_locations\": row_act_locations,\n",
    "    \"input_categories\": row_act_categories,\n",
    "    \"output_names\": col_act_names,\n",
    "    \"output_locations\": col_act_locations,\n",
    "    \"spearman\": spearman,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_)\n",
    "df.sort_values(by=\"spearman\", axis=0, ascending=False, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.to_excel(output_dir / \"GSA_results.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf177e1",
   "metadata": {},
   "source": [
    "# 5. Plot distributions of important inputs\n",
    "\n",
    "Could be helpful for interpreting GSA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867821cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm, triang\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_distribution(exchange, samples, num_bins=60):\n",
    "    bin_min = np.percentile(samples, 1)   # Remove outliers\n",
    "    bin_max = np.percentile(samples, 99)  # Remove outliers\n",
    "    bins = np.linspace(bin_min, bin_max, num_bins+1, endpoint=True)\n",
    "    width = (bins[1]-bins[0])*0.8\n",
    "    midbins = (bins[1:]+bins[:-1])/2\n",
    "    \n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "    # Plot distribution samples\n",
    "    Y_samples, _ = np.histogram(samples, bins=bins, density=True)\n",
    "    plt.bar(\n",
    "        midbins,\n",
    "        Y_samples,\n",
    "        width=width,\n",
    "        label=\"Samples\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    # Plot defined pdf of the distribution\n",
    "    if exchange.uncertainty_type.id == 2:\n",
    "        loc, scale = exchange['loc'], exchange['scale']\n",
    "        Y_distr = lognorm.pdf(midbins, s=scale, scale=np.exp(loc))\n",
    "    elif exchange.uncertainty_type.id == 5:\n",
    "        mode, min_, max_ = exchange['loc'], exchange['minimum'], exchange['maximum']\n",
    "        c = (mode-min_)/(max_-min_)\n",
    "        loc = min_\n",
    "        scale = max_-min_\n",
    "        Y_distr = triang.pdf(midbins, c=c, loc=loc, scale=scale)\n",
    "    plt.plot(\n",
    "        midbins,\n",
    "        Y_distr,\n",
    "        \"r-\",\n",
    "        label=\"Defined distribution\",\n",
    "    )\n",
    "    plt.title(exchange['name'])\n",
    "    plt.xlabel(f\"Exchange amount, {exchange.unit}\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b036f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4  # You can select here an input you are interested in\n",
    "\n",
    "data_ranked = data[:,spearman_argsort]\n",
    "\n",
    "col_id = inds_ranked[i]['col']\n",
    "row_id = inds_ranked[i]['row']\n",
    "exchanges = list(bd.get_activity(col_id).exchanges())\n",
    "for exc in exchanges:\n",
    "    if exc.input.id == row_id:\n",
    "        break  \n",
    "samp = data_ranked[:,i]\n",
    "plot_distribution(exc, samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad51ab",
   "metadata": {},
   "source": [
    "# 6. Validation of GSA results\n",
    "\n",
    "How do we know that GSA results are correct? Unfortunately, this step is often ignored.\n",
    "\n",
    "<img src=\"input_data/validation.png\" width=500 style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "iterations_val = 200\n",
    "\n",
    "# === All foreground inputs vary ===\n",
    "\n",
    "# Here we will reuse LCA scores obtained for GSA\n",
    "all_path = output_dir / f\"lca_scores_{seed_gsa}_{iterations_gsa}_gsa.json\"        \n",
    "with open(all_path, 'r') as f:\n",
    "    lca_scores_all = json.load(f)\n",
    "lca_scores_all = np.array(lca_scores_all)[:iterations_val]\n",
    "\n",
    "        \n",
    "# === Only influential foreground inputs vary ===\n",
    "\n",
    "ninfs = np.arange(1, 31, 3)\n",
    "metric = []\n",
    "\n",
    "for ninf in ninfs:\n",
    "    \n",
    "    print(ninf)\n",
    "\n",
    "    # Create validation datapackage when only influential inputs vary\n",
    "    mask_inf = spearman <= spearman[spearman_argsort[ninf]]\n",
    "    data_inf = deepcopy(data)\n",
    "    data_inf[:,mask_inf] = static_data[mask_inf]\n",
    "\n",
    "    ntech = len(dp_gsa_tech.data[0])\n",
    "    name = \"validation\"\n",
    "    \n",
    "    dp_val_inf = bwp.create_datapackage(\n",
    "        name=name,\n",
    "        sequential=True,\n",
    "    )\n",
    "    dp_val_inf.add_persistent_array(\n",
    "        matrix=f\"technosphere_matrix\",\n",
    "        data_array=data_inf[:, :ntech].T,\n",
    "        name=f\"{name}_tech\",\n",
    "        indices_array=dp_gsa_tech.data[0],\n",
    "        flip_array=dp_gsa_tech.data[2],\n",
    "    )\n",
    "    dp_val_inf.add_persistent_array(\n",
    "        matrix=f\"biosphere_matrix\",\n",
    "        data_array=data_inf[:, ntech:].T,\n",
    "        name=f\"{name}_bio\",\n",
    "        indices_array=dp_gsa_bio.data[0],\n",
    "    )\n",
    "    dps_inf = [me, bs, ei_nounct, wa_nounct, li_nounct, dp_val_inf]\n",
    "\n",
    "    # Run MC\n",
    "    lca_inf = bc.LCA(\n",
    "        {demand_act.id: 1}, \n",
    "        data_objs=dps_inf,\n",
    "        use_distributions=False, \n",
    "        use_arrays=True,\n",
    "    )\n",
    "    lca_inf.lci()\n",
    "    lca_inf.lcia()\n",
    "    \n",
    "    inf_path = output_dir / f\"lca_scores_{seed_gsa}_{iterations_val}_validation_inf_{ninf}.json\"\n",
    "    if inf_path.exists():\n",
    "        # Read LCIA scores\n",
    "        with open(inf_path, 'r') as f:\n",
    "            lca_scores_inf = json.load(f)\n",
    "    else:\n",
    "        lca_scores_inf = [lca_inf.score]\n",
    "        for i in range(iterations_val-1):\n",
    "            next(lca_inf)\n",
    "            lca_scores_inf.append(lca_inf.score)\n",
    "        # Save LCIA scores\n",
    "        with open(inf_path, 'w') as f:\n",
    "            json.dump(lca_scores_inf, f)\n",
    "            \n",
    "    metric.append(spearmanr(lca_scores_inf, lca_scores_all)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a786841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "plt.plot(\n",
    "    ninfs,\n",
    "    metric,\n",
    "    \".-\",\n",
    "    label=\"Spearman\",\n",
    ")\n",
    "\n",
    "plt.title(\"GSA validation curve\")\n",
    "plt.xlabel(\"# influential inputs\")\n",
    "plt.ylabel(\"Spearman correlation R(Yall, Yinf)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca08ec",
   "metadata": {},
   "source": [
    "## Validation plot for a specific number of influential inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninf = 16\n",
    "\n",
    "iterations_plot = 50\n",
    "\n",
    "# Read LCIA scores\n",
    "inf_path = output_dir / f\"lca_scores_{seed_gsa}_{iterations_val}_validation_inf_{ninf}.json\"\n",
    "with open(inf_path, 'r') as f:\n",
    "    lca_scores_inf = json.load(f)\n",
    "    \n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(iterations_plot),\n",
    "    lca_scores_all[:iterations_plot],\n",
    "    \".-\",\n",
    "    label=\"All inputs vary\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(iterations_plot),\n",
    "    lca_scores_inf[:iterations_plot],\n",
    "    \"x-\",\n",
    "    label=\"Only influential inputs vary\",\n",
    ")\n",
    "\n",
    "plt.title(\"GSA validation\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"LCIA scores, kg CO2-eq\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8148f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a095a-ddb9-49b8-be99-7518b6bd5b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:bw25]",
   "language": "python",
   "name": "conda-env-bw25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
